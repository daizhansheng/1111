[TOC]

# 1.线性回归

## 1.1线性回归起源

回归分析最早是19世纪末期高尔顿（Sir Francis Galton）所发展。高尔顿是生物统计学派的奠基人，他的表哥达尔文的巨著《物种起源》问世以后，触动他用统计方法研究智力进化问题，统计学上的“相关”和“回归”的概念也是高尔顿第一次使用的。

1855年，他发表了一篇“遗传的身高向平均数方向的回归”文章，分析儿童身高与父母身高之间的关系，发现父母的身高可以预测子女的身高，当父母越高或越矮时，子女的身高会比一般儿童高或矮，他将儿子与父母身高的这种现象拟合出一种线形关系。但是有趣的是：通过观察他注意到，尽管这是一种拟合较好的线形关系，但仍然存在例外现象：矮个的人的儿子比其父要高，身材较高的父母所生子女的身高将回降到人的平均身高。换句话说，当父母身高走向极端（或者非常高，或者非常矮）的人的子女，子女的身高不会象父母身高那样极端化，其身高要比父母们的身高更接近平均身高。高尔顿选用“回归”一词，把这一现象叫做“向平均数方向的 回归”（regression toward mediocrity）。

而关于父辈身高与子代身高的具体关系是如何的，高尔顿和他的学生K·Pearson观察了1078对夫妇，以每对夫妇的平均身高作为自变量，取他们的一个成年儿子的身高作为因变量，结果发现两者近乎一条直线，其回归 直线方程为：y^=33.73+0.516x ，这种趋势及回归方程表明父母身高每增加一个单位时，其成年儿子的身高平均增加0.516个单位。这样当然极端值就会向中心靠拢。

## 1.2线性回归的作用

*回归*（regression）是能为一个或多个自变量与因变量之间关系建模的一类方法。 在自然科学和社会科学领域，回归经常用来表示输入和输出之间的关系。

在机器学习领域中的大多数任务通常都与*预测*（prediction）有关。 当我们想预测一个数值时，就会涉及到回归问题。 常见的例子包括：预测价格（房屋、股票等）、预测住院时间（针对住院病人等）、 预测需求（零售销量等）。 但不是所有的*预测*都是回归问题。 在后面的章节中，我们将介绍分类问题。分类问题的目标是预测数据属于一组类别中的哪一个。

> 1. 预测值定量分析，即连续变量预测时，我们称之为*回归*（比如预测房价）
> 2. 预测值定性分析，即离散变量预测时，我们称之为*分类*（比如判断一张图片是不是dog）

## 1.3线性回归使用实例

### 1.3.1线性回归实例简介

我们以一个简单的房价预测作为例子来解释线性回归的基本要素及实现方法。以房子==面积==、离地铁站的==距离==等属性作为自变量，==房价==是因变量：

- 房子面积
- 离最近地铁站距离
- 绿化规模

一元线性回归模型：

$h_\theta({x}) = \theta_0x_0+\theta_1x_1$ $(x_0=1)$

$$y=kx+b$$

多元线性回归模型：

$h_\theta({x}) = \theta_0x_0+\theta_1x_1+...+\theta_nx_n = \sum_{i=0}^{n}\theta_ix_i = \theta^TX$ $(x0=1)$

线性回归可以理解为将这些特征线性组合起来，即可得到房价$h_\theta(x)$。而我们的目标就是求出一组合理的权重参数$\theta$能够较为准确的预测出真实的$h_\theta(x)$。

$$y=kx+b$$

这里x是年份，y是该年份的房价

我们采用一元线性回归模型来拟合上海市房价，通过已知历史数据拟合出k和b的值（模型的参数），然后我们就可以通过k、b来估算2024年、2025年房价，如2024年预计10.8万元。

这里给定2个样本：年份x，和对应的房价y， 设计函数：

$$L(x^{(1)}) = \frac{1}{2}[kx^{(1)}+b]-y^{(1)}]^2$$

$$L(x^{(2)}) = \frac{1}{2}[kx^{(2)}+b]-y^{(2)}]^2$$

$$L = \frac{L(x^{(1)})+L(x^{(2)})}{2}$$

此时L取0时，对应的k和b就是我们要的结果，然后使用梯度下降法，求解函数极值，得到合适的k和b，就是线性回归要做的事情。

> 这里设计的L函数，我们称之为损失函数/代价函数
> 
> 在机器学习中，将衡量误差的函数称为损失函数，而如$L=\frac{1}{2}(h_\theta(x)-y)^2$形式的函数，我们称之为平方损失(MSE/L2Loss)。

### 1.3.2怎么理解MSE(损失函数)

对于只有2个样本的时候，$y=kx+b$有唯一解，但是当具有大于2个样本时，k和b只有近似解，也就是说，不可能通过k和b完全预测准确，总会带有一些差异 因此真实值与预测值的关系如下表示：

<img src="https://hqyj-note-picture.oss-cn-beijing.aliyuncs.com/picture_bak/mypicture202312071147019.png" alt="img" width=300/> 

而我们总希望预测值足够的接近真实值，也意味着希望不可控的误差值足够接近0，因此需要对误差项进行分析。 假设误差是服从均值为0方差为$\delta^2$的正态分布：

为什么这里均值为0，因为均值是预测值。为什么是正态分布，现实生活中绝大部分事物都很容易的服从正态分布。这是统计出来的规律做的假设，使得对误差的假设能够尽可能满足更多的场景。若数据不满足正态分布，则假设失败，模型容易出现非预期结果。

态分布函数，又称为高斯分布函数，是描述连续型随机变量的概率分布函数之一。正态分布以其钟形曲线而闻名，具有以下概率密度函数（Probability Density Function，PDF）：

$ f(x)=\frac{1}{σ\sqrt{2π}}exp⁡^{-\frac{(x−μ)^2}{2σ^2}}$

其中：

- *x* 是随机变量的值。
- *μ* 是均值（期望值）。
- *σ* 是标准差。
- *π* 是圆周率，约等于3.14159。
- ⁡exp 是指数函数。

对上述的表达式两边取对数的结果如下：

$$ln^{f(x)} = ln^{\frac{1}{σ\sqrt{2π}}}-{\frac{(x−μ)^2}{2σ^2}}$$

在上述的表达式中前半部分都是固定的值，只有最后一个快是变量，所以损失函数如下：

$$J(\theta) = \frac{1}{2}(y_i-h_\theta(x))^2$$

> 注：
> 
> 可以不取 $ \frac{1}{2} $。在实际应用中，取  $ \frac{1}{2} $是为了方便后续计算梯度时的简化。然而，从最优化的角度来看，最小化 $\frac{1}{2}(y\_i-h_\theta(x))^2 $ 和最小化 $(y\_i-h_\theta(x))^2 $ 是等价的，因为它们的最小值出现在相同的 $ \theta $ 处。
> 
> 如果去掉 $\frac{1}{2} $，那么损失函数将变为 $ (y\_i-h_\theta(x))^2 $。在梯度下降等优化算法中，导数计算时需要额外的系数，但最终的最优化结果不会受到影响。这只是为了数学上的方便和简化。因此，使用 $(y\_i-h_\theta(x))^2 $ 或 $ \frac{1}{2}(y\_i-h\_\theta(x))^2 $ 都是合理的，只是在具体计算梯度时要注意相应的系数。
> 

# 2.梯度下降法

梯度下降是一种优化算法，用于最小化一个函数，通常是损失函数。梯度下降的目标是找到函数的局部最小值或全局最小值，通过不断调整参数来降低函数值。

梯度下降的核心思想是利用函数的梯度（或导数）信息，沿着梯度的负方向更新参数，以减小函数值。详细过程如下：

先从最简单的凸函数$f(x) = x^2$ 开始讲起。

<img title="" src="https://hqyj-note-picture.oss-cn-beijing.aliyuncs.com/picture_bak/mypicture202312071414409.png" alt="image-20231130112256067" width="454"> 

假设起点在x0=10 处，也就是将球放在x0=10 ：

<img title="" src="https://hqyj-note-picture.oss-cn-beijing.aliyuncs.com/picture_bak/mypicture202312071414924.png" alt="image-20231130113412697" width="456"> 

它的梯度为1维向量：

$$\nabla f(x_0)=f'(x_0)*i)i=(f'(x_0))=(2x|x_0=10) = (20)$$

这是x轴上的向量，它指向函数值增长的最快的方向，而$-\nabla f(x_0)$就指向减少最快的方向：

<img title="" src="https://hqyj-note-picture.oss-cn-beijing.aliyuncs.com/picture_bak/mypicture202312071414767.png" alt="image-20231130134057480" width="459"> 

将x\~0\~也看做1维向量(x\~0\~),通过和$- \nabla f(x_0)$相加，可以将之向$- \nabla f(x_0)$移动一段距离得到新的向量(x\~1\~):

$$(x_1) = (x_0)-ηf(x_0)$$

其中η称之为步长，通过它可以控制移动的距离，本节设η=0.2那么：

$$(x_1) = (x_0)-ηf(x_0)=(10)-0.2*(20)=(6)$$

此时小球（也就是启点）下降到x\~1\~ = 6的位置：

<img title="" src="https://hqyj-note-picture.oss-cn-beijing.aliyuncs.com/picture_bak/mypicture202312071414375.png" alt="image-20231130134136348" width="462"> 

迭代

x\~1\~的梯度为：(倒三角是梯度的意思)

$$\nabla f(x_1)=f'(x_1)i=(f'(x_1))=(2x|x_1=6) = (12)$$

继续沿着梯度的反方向走：

$$(x_2)=(x_1)-ηf(x_1)=(6)-0.2*(12)=(3.6)$$

<img title="" src="https://hqyj-note-picture.oss-cn-beijing.aliyuncs.com/picture_bak/mypicture202312071414771.png" alt="image-20231130132816065" width="464"> 

重复上述过程到第10次，小球基本上就到了最低点，既有$x_{10} \approx 0$

<img title="" src="https://hqyj-note-picture.oss-cn-beijing.aliyuncs.com/picture_bak/mypicture202312071414280.webp" alt="动图" width="488"> 

把每次的梯度向量$\nabla f$的模长|$\nabla f$|列出来，可以看到时再不断减小的，因此这种方法称之为梯度下降法

![image-20231130133645911][image-1]

> 梯度下降算法中的学习率 η，η（有时也被称为步长或学习速率）的大小对算法的性能有很大影响。以下是学习率大小对梯度下降的影响：
> 
> 1. **学习率过小：** 如果学习率太小，算法会收敛得非常缓慢。每次更新都只会微调参数，可能需要很长时间才能达到最优解，而且容易陷入局部最小值。
> 2. **学习率过大：** 如果学习率太大，可能会导致算法无法收敛，甚至发散。在每次迭代中，参数的更新可能会越过最优解，导致算法在参数空间中不断振荡或错过最小值。
> 3. **适中的学习率：** 选择适中的学习率是关键。合适的学习率使得算法能够在合理的时间内收敛到最优解。一般而言，可以通过实验和调整来选择适当的学习率。
> 
> <img src="https://hqyj-note-picture.oss-cn-beijing.aliyuncs.com/picture_bak/mypictureimage-20231130134420581.png" title="" alt="image-20231130134420581" width="685"> 

# 3.链式求导法

链式求导法则是一种用于计算复合函数导数的方法，它的基本思想是将复合函数的导数分解为各个组成函数的导数的乘积。复合函数是指一个函数的自变量是另一个函数的函数值，例如 $y=f(g(x))$ 就是一个复合函数，其中 $g(x)$ 是内层函数，$f(g(x))$ 是外层函数。链式求导法则的公式如下：

$$\frac{dy}{dx}=\frac{dy}{du}\frac{du}{dx}$$

其中 $u=g(x)$，$y=f(u)$，$\frac{dy}{du}$ 是外层函数对内层函数的导数，$\frac{du}{dx}$ 是内层函数对原自变量的导数。链式求导法则的几何意义是，当原自变量 $x$ 变化一个微小量 $dx$ 时，内层函数 $u$ 变化一个微小量 $du$，外层函数 $y$ 变化一个微小量 $dy$，这三个微小量之间的关系就是链式求导法则的公式。

链式求导法则的应用范围很广，它可以用于求解一元函数、多元函数、隐函数、反函数等复合函数的导数。下面举一些例子来说明链式求导法则的使用方法。

例1：求 $y=\sin^2x$ 的导数。

解：这个函数可以看作是 $y=u^2$ 和 $u=\sin x$ 的复合函数，其中 $u=\sin x$ 是内层函数，$y=u^2$ 是外层函数。根据链式求导法则，有

$$\frac{dy}{dx}=\frac{dy}{du}\frac{du}{dx}=2u\cos x=2\sin x\cos x$$

例2：求 $y=\sqrt{1+x^2}$ 的导数。

解：这个函数可以看作是 $y=\sqrt{u}$ 和 $u=1+x^2$ 的复合函数，其中 $u=1+x^2$ 是内层函数，$y=\sqrt{u}$ 是外层函数。根据链式求导法则，有

$$\frac{dy}{dx}=\frac{dy}{du}\frac{du}{dx}=\frac{1}{2\sqrt{u}}\cdot 2x=\frac{x}{\sqrt{1+x^2}}$$

例3：求 $y=\ln(\cos x)$ 的导数。

解：这个函数可以看作是 $y=\ln u$ 和 $u=\cos x$ 的复合函数，其中 $u=\cos x$ 是内层函数，$y=\ln u$ 是外层函数。根据链式求导法则，有

$$\frac{dy}{dx}=\frac{dy}{du}\frac{du}{dx}=\frac{1}{u}\cdot (-\sin x)=-\frac{\sin x}{\cos x}=-\tan x$$


[image-1]:	https://hqyj-note-picture.oss-cn-beijing.aliyuncs.com/picture_bak/mypicture202312071414887.png
[TOC]



# 1.人工神经网络ANN

## 1.1ANN简介

人工神经网络(Artificial Neural Network, ANN)，简称神经网络，是一种模仿生物神经网络（动物的中枢神经系统，特别是大脑）的结构和功能的数学模型或计算模型。人工神经网络模型主要考虑网络连接的拓扑结构、神经元的特征、学习规则等。目前，已有近 40 种神经网络模型，其中有反传网络、感知器、自组织映射、Hopfield 网络、玻尔兹曼机等等。

根据网络连接的拓扑结构，神经网络模型可以分为：
(1) **前向神经网络（又称前馈神经网络）** 。网络中各个神经元接受前一级的输入，并输出到下一级，网络中没有反馈，可以用一个有向无环路图表示。这种网络实现信号从输入空间到输出空间的变换，它的信息处理能力来自于简单非线性函数的多次复合。网络结构简单，易于实现。反向传播神经网络是一种典型的前向网络。
(2) **反馈神经网络** 。网络内神经元间有反馈，可以用一个无向的完备图表示。这种神经网络的信息处理是状态的变换，可以用动力学系统理论处理。系统的稳定性与联想记忆功能有密切关系。Hopfield 网络、玻尔兹曼机均属于这种类型。

<img src="https://hqyj-note-picture.oss-cn-beijing.aliyuncs.com/picture_bak/mypicture202312051142537.gif" alt="ANN_history.gif" style="zoom: 67%;" /> 

## 1.2神经元和感知层

神经元(neuron)，又称神经细胞，是神经系统的结构与功能单位之一，如图 

![ANN_neuron.gif](https://hqyj-note-picture.oss-cn-beijing.aliyuncs.com/picture_bak/mypicture202312051144202.gif) 

神经元有两个状态：兴奋和抑制。神经元的状态取决于从树突和细胞体接受到的信号，当信号量总和超过了某个阈值时，细胞体就会兴奋，产生电脉冲。电脉冲沿着“轴突”并通过“突触”传递到其它神经元。

感知器(Perceptron)是由美国科学家 Frank Rosenblatt 于 1957 年提出的， **感知器模拟了生物神经元的行为。** 感知器可用下面式子表示：

$f(\boldsymbol{x}) = sign(\boldsymbol{w} \cdot \boldsymbol{x} + b) = \begin{cases}
+1 & \text{if} \; \boldsymbol{w} \cdot \boldsymbol{x} + b > 0 \\
-1 & \text{otherwise}
\end{cases}$

<img src="https://hqyj-note-picture.oss-cn-beijing.aliyuncs.com/picture_bak/mypicture202312051146168.gif" alt="ANN_neuron_perceptron.gif" style="zoom:67%;" /> 

## 1.3神经网络的结构

![image-20231130101152016](https://hqyj-note-picture.oss-cn-beijing.aliyuncs.com/picture_bak/mypictureimage-20231130101152016.png) 

输入层神经元：输入层神经元主要是用来接收传递过来的原始数据，它们将数据按照配置的权重传递个隐藏层。

隐藏层：隐藏层是神经网络的中间层，介于输入层和输出层之间，隐藏层可以有一层也可以有多层。

> 隐藏层的简介：
> 
> 1. **特征提取与表示：** 隐藏层的主要作用之一是从原始数据中提取和学习更高级别的特征。通过组合输入层的特征，隐藏层的神经元可以学习表示输入数据的更复杂的模式和结构。
> 2. **非线性映射：** 隐藏层引入非线性激活函数，允许神经网络学习非线性关系。线性模型受限于只能学习线性关系，而隐藏层的非线性映射能够使神经网络更灵活地适应复杂的数据模式。
> 3. **降维和压缩：** 在某些情况下，隐藏层可以通过降维和压缩输入数据，减少模型的复杂度。这有助于去除冗余信息，提高模型的泛化能力，并减少过拟合的风险。
> 4. **学习复杂任务：** 隐藏层的存在使神经网络能够学习解决复杂任务，如图像识别、自然语言处理等。通过多个隐藏层，网络可以逐渐学习复杂的抽象表示，从而提高对任务的理解和性能。
> 5. **减少计算复杂度：** 隐藏层的使用可以减少输入层与输出层之间的连接数量。这对于处理大规模数据和复杂模型时，有助于减少计算和存储的需求。
> 6. **防止过拟合：** 隐藏层的存在提供了一种正则化机制，可以帮助防止模型对训练数据的过拟合。隐藏层通过减少模型的复杂性，有助于提高模型对未见过数据的泛化能力。
> 7. **梯度下降的深化：** 多层的隐藏层允许神经网络实现深度学习。深度学习模型通过多层次的特征学习可以更好地适应复杂的任务，从而提高模型的性能。

输出层：神经网络的输出层是网络的最后一层，负责生成模型的最终输出

## 1.4激活函数

### 1.4.1激活函数简介

当输入层输入数据后通过分配的权重传递给隐藏层的对应的神经元，隐藏层的那些神经元被激活？这里就要涉及到激活函数。

拿其中一个神经元为例，其信号强度取决于上层传递的加权和所决定。激活函数会将输入信号转化成该神经元的输出值。

<img title="" src="https://hqyj-note-picture.oss-cn-beijing.aliyuncs.com/picture_bak/mypictureimage-20231130101931907.png" alt="image-20231130101931907" width="699"> 

假如该激活函数是简单的分段函数，当输入信号强度低于预设的临界值时输出值为0，当输入的信号强度大于预设的临界值的时候该神经元被激活。这样会带来一个问题当输入值逐渐增大到临界值时，输出的结果就会从0到1进行突变。为了平滑神经元的输出值往往会采用不同的激活函数。

<img src="https://hqyj-note-picture.oss-cn-beijing.aliyuncs.com/picture_bak/mypictureimage-20231130102719424.png" title="" alt="image-20231130102719424" width="707"> 

### 1.4.2Sigmoid激活函数

Sigmoid函数是一种常用的激活函数，通常用于神经网络中的二分类问题。sigmoid函数输出

值落在0-1的区间内，并呈现出s形的曲线。公式如下：
$$
Sidmoid=\frac{1}{1+e^{-x}}
$$
其中，e是自然对数的底（约等于2.71828），而 x是输入。 

1. **取值范围：** Sigmoid函数的输出范围在 (0, 1) 之间。当 x接近正无穷大时，$e^{−x}$趋近于0，因此分母趋近于1，函数值趋近于1；当 x接近负无穷大时，$e^{−x}$趋近于无穷大，因此分母趋近于无穷大，函数值趋近于0。

2. **平滑性：** Sigmoid函数在整个实数范围内是连续且可导的。这种平滑性对于梯度下降等优化算法很有用，因为它可以帮助模型更好地学习。

3. **梯度：** Sigmoid函数的导数可以通过链式法则计算。如果 σ(x)是Sigmoid函数，那么它的导数 σ′(x)可以表示为：
   
   $σ′(x)=σ(x)⋅(1−σ(x))$
   
   这个导数在 x=0 处达到最大值，为0.25。这也是为什么在深度学习中，初始化权重时要考虑到Sigmoid函数的梯度饱和问题。

4. **应用：** Sigmoid函数广泛应用于二分类问题，例如在输出层对二分类神经网络的输出进行压缩到 (0, 1) 范围内，表示概率。然而，对于深度神经网络，有时候会使用其他激活函数如ReLU，因为它在处理梯度消失问题上效果更好。

**Sigmoid函数求导过程**：

它的形式是 $S(x)=\frac{1}{1+e^{-x}}$，它的值域是 $(0,1)$，它的图像是一个S形曲线。Sigmoid函数的求导过程可以用链式法则来推导，具体步骤如下：

- 首先，令 $y=1+e^{-x}$，则 $S(x)=\frac{1}{y}$
- 对 $S(x)$ 求导，得 $S'(x)=-\frac{1}{y^2}y'$
- 对 $y$ 求导，得 $y'=-e^{-x}$
- 代入 $y'$，得 $S'(x)=\frac{e^{-x}}{y^2}$
- 代入 $y$，得 $S'(x)=\frac{e^{-x}}{(1+e^{-x})^2}$
- 化简，得 $S'(x)=\frac{1}{1+e^{-x}}\frac{e^{-x}}{1+e^{-x}}$ 注:($e^{-x}=\frac {1-f(x)}{f(x)}$即：$\frac{e^{-x}}{1+e^{-x}}$等于$\frac {1-f(x)}{f(x)} * f(x) = (1-f(x))$)
- 由于 $S(x)=\frac{1}{1+e^{-x}}$，所以 $S'(x)=S(x)(1-S(x))$

这就是Sigmoid函数的求导结果，它表明Sigmoid函数的导数是以它本身为因变量的函数。sigmoid和其导数图像。

<img src="https://hqyj-note-picture.oss-cn-beijing.aliyuncs.com/picture_bak/mypicture202312041024489.png" title="" alt="" width="820"> 

> **Sigmod梯度消失问题：**
> 
> Sigmoid函数$Sigmoid = \frac{1}{1+e^{-x}}$导数为$σ′(x)=σ(x)⋅(1−σ(x))$
> 
> sigmoid的导数落在0-0.25之间，当隐藏层比较多的时候，根据链式求导法则0.25\*0.25\*0.25\*......0.25≈0。这种现象我们就称之为梯度消失。

### 1.4.3Tanh激活函数

tanh(双曲正切)函数可以将元素的值落在-1到1之间：

$$
tanh = \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}} = \frac{(e^{x}-e^{-x})*(e^{-x})}{(e^{x}+e^{-x})*(e^{-x})} = \frac{1-e^{-2x}}{1+e^{-2x}}
$$

tanh函数的曲线图如下：

<img title="" src="https://hqyj-note-picture.oss-cn-beijing.aliyuncs.com/picture_bak/mypicture202312041038944.png" alt="" width="558"> 

tanh函数的求导过程如下：

$tanh' = [\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}]'$例如如下求导法则:

$[\frac{f(x)}{g(x)}]' = \frac{f'(x)g(x) + f(x)g'(x)}{g^2(x)}$结果如下：

$\frac{d}{dx} \tanh(x) = \frac{d}{dx} \left(\frac{e^x - e^{-x}}{e^x + e^{-x}}\right) = \frac{(e^x + e^{-x})(e^x + e^{-x}) - (e^x - e^{-x})(e^x - e^{-x})}{(e^x + e^{-x})^2}$

$\frac{d}{dx} \tanh(x) = 1 - \left(\frac{e^x - e^{-x}}{e^x + e^{-x}}\right)^2 = 1 - \tanh^2(x)$

tanh函数导数曲线图如下：

<img src="https://hqyj-note-picture.oss-cn-beijing.aliyuncs.com/picture_bak/mypicture202312041118272.png" title="" alt="" width="574"> 

tanh函数的导数落在0-1区间，在进行链式求导的时候效果是要优于sigmoid函数的，但是层数多了之后依然有可能出现梯度消失。

### 1.4.4ReLU激活函数

ReLU（Rectified Linear Unit）激活函数是神经网络中常用的非线性激活函数
$$
f(x)=max(0,x)
$$
Relu函数的图像和求导后的图像如下：

<img title="" src="https://hqyj-note-picture.oss-cn-beijing.aliyuncs.com/picture_bak/mypicture202312041126485.png" alt="" width="654"> 

> **ReLU激活函数有什么优势**：
> 
> 1.没有饱和区，不存在梯度消失问题。  
> 2.没有复杂的指数运算，计算简单、效率提高。  
> 3.实际收敛速度较快，比 Sigmoid/tanh 快很多。  
> 4.比 Sigmoid 更符合生物学神经激活机制。  
> **ReLU激活函数的不足**：
> 
> 训练的时候很”脆弱”，很容易就”die”了。如果你的learning rate 很大，那么很有可能你网络中的40%的神经元都”dead”了。此外，ReLU函数在负半区的导数为0，所以一旦神经元激活值进入负半区，那么梯度就会为0，而正值不变。
> 
> 为了解决ReLU函数的这些问题，人们提出了一些改进的ReLU函数，如Leaky ReLU和PReLU等3。这些函数在输入小于0的部分，值为负，且有微小的梯度3，从而避免了神经元”死亡“的问题。具体的选择需要根据实际的应用场景和数据集来决定。

### 1.4.5Leaky ReLU激活函数

Leaky ReLU（Leaky Rectified Linear Unit）是ReLU激活函数的一种改进版本。它的主要目标是解决ReLU函数的“死亡”神经元问题。

Leaky ReLU函数的表达式如下：

$f(x) = 
\begin{cases} 
x & \text{if } x > 0 \\
αx & \text{if } x \leq 0 
\end{cases}$

其中，α是一个很小的正数，通常取0.01。

这意味着，如果输入值大于0，Leaky ReLU函数会输出输入值本身；如果输入值小于或等于0，Leaky ReLU函数会输出α倍的输入值。这样，即使在输入值小于0的情况下，Leaky ReLU函数也能有一个非零的梯度。

Leaky ReLU函数的主要优点是：

1. **解决“死亡”神经元问题**：ReLU函数在输入小于0的部分，梯度为0，导致神经元无法学习。Leaky ReLU函数通过引入一个小的斜率，在负值区间上产生非零输出，避免了ReLU函数在负值区间输出为零的问题。
2. **保持ReLU函数的优点**：Leaky ReLU函数保留了ReLU函数的所有优点，包括解决梯度消失问题，引入稀疏性，计算简单，实际收敛速度较快等。

然而，Leaky ReLU函数也存在一些问题。例如，虽然它解决了ReLU函数的“死亡”神经元问题，但是在输入值大于0的部分，它的表现和ReLU函数一样。此外，Leaky ReLU函数的性能可能会受到参数α的影响。

<img src="https://hqyj-note-picture.oss-cn-beijing.aliyuncs.com/picture_bak/mypicture202312041246444.png" title="" alt="" width="675"> 

## 1.5逆向参数调整法

### 1.5.1逆向参数调整(两层)

假如下图的神经网络，x1和x2是输入层，H是隐藏层/输出层。通过[x~11~,x~12~,x~13~,x~14~...x~1n~]和[x~21~,x~22~,x~23~,x~24~...x~2n~]两组数据

通过神经网络得到结果集合[E~1~,E~2~,E~3~,E~4~...E~n~]。训练的目的就是通过不断调整w~1~和w~2~的权重，让结果集合最接近期望结果

[y~1~,y~2~,y~3~,y~4~...y~n~]，也就是让$ s = \sum_{i=1}^{n} (E_i - y_i)^2 $误差平方和最小化。这个调整的过程就是逆向参数调整法。具体的调整过程中

需要用到梯度下降法(Gradient Descent)和链式法则(Chain Rule)

<img src="https://hqyj-note-picture.oss-cn-beijing.aliyuncs.com/picture_bak/mypictureimage-20231130104007572.png" title="" alt="image-20231130104007572" width="696"> 

![image-20231130104758399](https://hqyj-note-picture.oss-cn-beijing.aliyuncs.com/picture_bak/mypictureimage-20231130104758399.png) 

梯度下降：

可以使用梯度下降算法求得误差最小的极值，梯度下降的解释见==梯度下法==章节。

-:导数为正时极值在它左侧，导数为负是极值在它右侧，所以刚好需要一个负号

η：代表的是步长（值要适中,太小学习的比较慢，太大可能会跳过极致设置发散）

$δs/δw_j$$:$代表的就是变化率

<img title="" src="https://hqyj-note-picture.oss-cn-beijing.aliyuncs.com/picture_bak/mypictureimage-20231130135114959.png" alt="image-20231130135114959"  width="560"> 

链式法则求导：==链式求导法则请看下面章节==

![image-20231130141553677](https://hqyj-note-picture.oss-cn-beijing.aliyuncs.com/picture_bak/mypictureimage-20231130141553677.png) 

经过上述链式求导法则得到如下公式,这就是权重的调整方法：

$Δ w_j=-η* 2*(E-y)*E*(1-E)*x_j$

上述公式的含义如下：

你可以把这个权重调整方法想象成一个**下山**的过程，你的目的是**找到**山谷中的**最低点**，也就是误差函数的**最小值**。你的位置是由你的**连接权重**决定的，你的高度是由你的**误差函数**决定的，你的方向是由你的**负梯度**决定的，你的步幅是由你的**学习率**决定的。

每次你要**更新**你的位置，也就是**调整**你的权重，你都要做以下几件事：

- 你要看看你现在的高度，也就是你的**误差函数**的值，它反映了你和最低点的**距离**，如果距离很远，就说明你还没有到达最低点，如果距离很近，就说明你已经接近最低点。
- 你要看看你现在的方向，也就是你的**负梯度**的方向，它反映了你下山的**最快**的方向，如果方向很明确，就说明你可以沿着这个方向继续下山，如果方向很模糊，就说明你可能已经到达了一个**平坦**的地方，也就是一个**局部最优**的点。
- 你要看看你现在的步幅，也就是你的**学习率**的大小，它反映了你每次下山的**距离**，如果步幅很大，就说明你可以快速地下山，但也有可能会**跳过**最低点，如果步幅很小，就说明你可以精确地下山，但也有可能会**收敛太慢**或**陷入**局部最优。

然后，你要根据你的高度，方向和步幅，来**计算**你的新的位置，也就是你的新的权重，这就是这个公式的作用：

$Δ w_j=-η* 2*(E-y)*E*(1-E)*x_j$

其中：

- $Δ w_j$ 是你的位置的**变化量**，也就是你的权重的**调整量**。
- $-η$ 是你的方向的**符号**，也就是你的负梯度的**方向**，它表示你要**沿着**误差函数下降的方向**移动**。
- $2*(E-y)$ 是你的高度的**变化率**，也就是你的误差函数的**偏导数**，它表示你和最低点的**距离**，如果距离很大，就说明你要**移动**更多，如果距离很小，就说明你要**移动**更少。
- $E*(1-E)$ 是你的方向的**变化率**，也就是你的负梯度的**偏导数**，它表示你下山的**最快**的方向，如果方向很明确，就说明你要**移动**更少，如果方向很模糊，就说明你要**移动**更多。
- $x_j$ 是你的位置的**贡献因子**，也就是你的输入节点的**输入值**，它表示你的位置是由哪些因素**决定**的，如果因素很大，就说明你要**移动**更多，如果因素很小，就说明你要**移动**更少。

这样，你就可以根据这个公式，不断地**更新**你的位置，也就是**调整**你的权重，直到你**找到**最低点，也就是误差函数的**最小值**，这样你的网络就可以达到**最优**的性能。

### 1.5.2逆向参数调整法（三层）

如果将输入层和隐藏层分开，可以理解为三层的神经网络。结构如下图所示：

![image-20231201125215807](https://hqyj-note-picture.oss-cn-beijing.aliyuncs.com/picture_bak/mypictureimage-20231201125215807.png) 

我们可以将上述神经网络的公式列举下，激活函数还是采用sigmoid函数。

$h(u_j) = w'_j*u_j$

$x_j = \frac{1}{1+e^{-h(u_j)}}$ <mark>x~j~是期望</mark>

$y(x_j) = w_j * x_j$

$E = \frac{1}{1+e^{-y{x_j}}}$ <mark>E是期望</mark>

$S = \sum_{i=1}^{n} (E-y)^2 $

上述给出了每层的的函数及期望，采用sigmoid作为激活函数。多上述的公式进行链式求导的结果如下：

<img title="" src="https://hqyj-note-picture.oss-cn-beijing.aliyuncs.com/picture_bak/mypictureimage-20231201125056840.png" alt="image-20231201125056840" width="624"> 

## 1.6偏差Bias

神经网络中的偏置（bias）是一个常数，它被添加到每个神经元的加权输入中。虽然它只是一个小的常数项，但却在神经网络的学习过程中起着重要的作用。

在神经网络中，每个神经元接收来自上一层神经元的输出，并使用它们的权重进行计算，然后将结果传递到下一层神经元。在这个计算过程中，偏置扮演了非常重要的角色。

偏置单元（bias unit），在有些资料里也称为偏置项（bias term）或者截距项（intercept term），它其实就是函数的截距，与线性方程 y=wx+b 中的 b 的意义是一致的。在 y=wx+b中，b表示函数在y轴上的截距，控制着函数偏离原点的距离，其实在神经网络中的偏置单元也是类似的作用。

总的来说，偏置在神经网络中的主要作用是：

1. **增加模型的灵活性**：偏置可以使得神经网络的决策边界不必经过原点，从而增加了模型的拟合能力。
2. **控制神经元的激活状态**：偏置实际上是对神经元激活状态的控制。例如，在sigmoid函数中，加入偏置后也是增加了函数的灵活性，提高了神经元的拟合能力。
3. **调整信息的丢弃率**：偏置这个参数，就是调整丢弃的局部信息的比例或者能量，没有这个参数，对信息的抛弃率的调整的灵活性就欠缺。

## 1.7人工神经网络ANN代码实例

```python
# 导入相关库
import numpy as np
import matplotlib.pyplot as plt
import time


# 定义激活函数
def sigmoid(x):
    return 1 / (1 + np.exp(-x))


# 1.散点输入
points = np.array(
    [[0.8, 0], [1.1, 0], [1.7, 0], [1.9, 0], [2.7, 1], [3.2, 1], [3.7, 1], [4.0, 1], [5.0, 0], [5.5, 0], [6.0, 0],
     [6.3, 0]])
# 将输入特征和标签值分开（x是输入特征，y是真实标签值）
X = points[:, 0].reshape(-1, 1)
y = points[:, 1].reshape(-1, 1)
print(X)
print(y)
# 计算特征值的个数，反向传播求导时用
l_x_data = X.shape[0]

# 初始化图
plt.ion()  # 开启交互模式
plt.xlabel("x axis label")
plt.ylabel("y axis label")
# 定义图像的横坐标范围
x_values = np.linspace(0, 7, 100)

# 参数初始化
w11_1 = 0.5
b1_1 = 0.2
w12_1 = 0.3
b2_1 = 0.2
w11_2 = 0.1
w21_2 = 0.1
b1_2 = 0.2

# 超参数初始化(学习率和迭代次数)
learning_rate = 0.2

# 损失值列表，作图时使用
loss_values = []

# 4.开始迭代
num_iterations = 60000
for n in range(1, num_iterations + 1):
    # 2.前向计算
    z1_1 = w11_1 * X + b1_1
    a1_1 = sigmoid(z1_1)
    z2_1 = w12_1 * X + b2_1
    a2_1 = sigmoid(z2_1)
    z1_2 = w11_2 * a1_1 + w21_2 * a2_1 + b1_2
    a1_2 = sigmoid(z1_2)

    # 3.损失函数(均方差损失函数)
    e = np.mean((y - a1_2) ** 2)
    # 将损失值放入损失值列表中，方便图像显示
    loss_values.append(e)

    # 5.反向传播(对所有的参数进行求导)
    deda1_2 = -2 * (y - a1_2)
    dedz1_2 = deda1_2 * a1_2 * (1 - a1_2)

    dedw11_2 = (dedz1_2 * a1_1).sum() / l_x_data
    dedw21_2 = (dedz1_2 * a2_1).sum() / l_x_data
    dedb1_2 = (dedz1_2).sum() / l_x_data

    deda1_1 = dedz1_2 * w11_2
    dedz1_1 = deda1_1 * a1_1 * (1 - a1_1)
    dedw11_1 = (dedz1_1 * X).sum() / l_x_data
    dedb1_1 = (dedz1_1).sum() / l_x_data

    deda2_1 = dedz1_2 * w21_2
    dedz2_1 = deda2_1 * a2_1 * (1 - a2_1)
    dedw12_1 = (dedz2_1 * X).sum() / l_x_data
    dedb2_1 = (dedz2_1).sum() / l_x_data

    # 梯度下降更新参数
    w11_2 = w11_2 - learning_rate * dedw11_2
    w21_2 = w21_2 - learning_rate * dedw21_2
    b1_2 = b1_2 - learning_rate * dedb1_2

    w11_1 = w11_1 - learning_rate * dedw11_1
    b1_1 = b1_1 - learning_rate * dedb1_1
    w12_1 = w12_1 - learning_rate * dedw12_1
    b2_1 = b2_1 - learning_rate * dedb2_1

    # 6.显示频率设置
    frequency_display = 100
    if n % frequency_display == 0 or n == 1:
        # 绘制拟合曲线
        z1_1_values = w11_1 * x_values + b1_1
        a1_1_values = sigmoid(z1_1_values)
        z2_1_values = w12_1 * x_values + b2_1  # 更新 z2_1 的计算，使用 b2_1
        a2_1_values = sigmoid(z2_1_values)
        z1_2_values = w11_2 * a1_1_values + w21_2 * a2_1_values + b1_2
        y_values = sigmoid(z1_2_values)

        plt.clf()  # 清空当前子图内容
        # 绘制第一个子图，内容为散点拟合曲线图
        plt.subplot(2, 1, 1)
        # 定义标题为当前步数值
        plt.title(f'Step: {n}')
        # 绘制散点
        plt.scatter(X[:, 0], y, color='red', label='Original points')
        # 绘制曲线
        plt.plot(x_values, y_values, color='blue', label='Fitted curve')
        # 显示图像中散点和曲线对应的标签
        plt.legend()

        # 绘制第二个子图，内容为损失函数曲线图
        plt.subplot(2, 1, 2)
        # 绘制曲线
        plt.plot(range(len(loss_values)), loss_values, color='green')
        # 定义横坐标的标签
        plt.xlabel('Iteration')
        # 定义纵坐标的标签
        plt.ylabel('Loss')
        # 定义标题为损失值
        plt.title('Loss Curve')
        # 定义两个子图之间的垂直距离
        plt.subplots_adjust(hspace=0.5)
        # 显示图形
        plt.pause(0.1)  # 暂停0.1秒，让图形显示出来
input() #让程序在这里阻塞

```

<img src="https://hqyj-note-picture.oss-cn-beijing.aliyuncs.com/picture_bak/mypicture202312051737663.png" alt="image-20231205173702661" style="zoom:67%;" /> 

[TOC]

# 1.Softmax

## 1.1softmax使用场景

回归可以用于预测*多少*的问题。 比如预测房屋被售出价格，或者棒球队可能获得的胜场数，又或者患者住院的天数。

事实上，我们也对*分类*问题感兴趣：不是问“多少”，而是问“哪一个”：

- 某个电子邮件是否属于垃圾邮件文件夹？
- 某个用户可能*注册*或*不注册*订阅服务？
- 某个图像描绘的是驴、狗、猫、还是鸡？
- 某人接下来最有可能看哪部电影？

通常，机器学习实践者用*分类*这个词来描述两个有微妙差别的问题： 1. 我们只对样本的“硬性”类别感兴趣，即属于哪个类别； 2. 我们希望得到“软性”类别，即得到属于每个类别的概率。 这两者的界限往往很模糊。其中的一个原因是：即使我们只关心硬类别，我们仍然使用软类别的模型。

## 1.2softmax网络架构

让我们考虑一个简单的图像分类问题，其输入图像的高和宽均为2像素，且色彩为灰度。这样每个像素值都可以用一个标量表示。我们将图像中的4像素分别记为x1,x2,x3,x4。假设训练数据集中图像的真实标签为狗、猫或鸡（假设可以用4像素表示出这3种动物），这些标签分别对应离散值y1,y2,y3。

我们通常使用离散的数值来表示类别，例如y1=1,y2=2,y3=3,3。如此，一张图像的标签为1、2和3这3个数值中的一个。虽然我们仍然可以使用回归模型来进行建模，并将预测值就近定点化到1、2和3这3个离散值之一，但这种连续值到离散值的转化通常会影响到分类质量。因此我们一般使用更加适合离散值输出的模型来解决分类问题。



softmax回归跟线性回归一样将输入特征与权重做线性叠加。与线性回归的一个主要不同在于，softmax回归的输出值个数等于标签里的类别数。因为一共有4种特征和3种输出动物类别，所以权重包含12个标量（带下标的w）、偏差包含3个标量（带下标的b），且对每个输入计算o1,o2,o3这3个输出：

![image-20231207160713823](https://hqyj-note-picture.oss-cn-beijing.aliyuncs.com/picture_bak/mypicture202312071607900.png) 

我们可以用神经网络图(下图)来描述这个计算过程。 与线性回归一样，softmax回归也是一个单层神经网络。 由于计算每个输出$o_1$、$o_2$和$o_3$取决于 所有输入$x_1$、$x_2$、$x_3$和$x_4$， 所以softmax回归的输出层也是全连接层。

![image-20231207160816559](https://hqyj-note-picture.oss-cn-beijing.aliyuncs.com/picture_bak/mypicture202312071608638.png) 

为了更简洁地表达模型，我们仍然使用线性代数符号。 通过向量形式表达为$o=wx+b$， 这是一种更适合数学和编写代码的形式。 由此，我们已经将所有权重放到一个3×4矩阵中。 对于给定数据样本的特征$x$， 我们的输出是由权重与输入特征进行矩阵-向量乘法再加上偏置$b$得到的。

![image-20231207164107698](https://hqyj-note-picture.oss-cn-beijing.aliyuncs.com/picture_bak/mypicture202312071641821.png)  

## 1.3softmax的简介

Softmax 是一种用于==多类别分类问题的激活函数==，它将输入转换成一个表示概率分布的输出。Softmax 主要应用于神经网络的输出层，特别是在多类别分类任务中。

Softmax 函数的定义如下：

给定输入向量 z=(z1,z2,…,zk)，Softmax 函数将每个元素$z_i$转换为一个介于 0 和 1 之间的值，并且所有元素的和为 1，即：

$ \text{Softmax}(z)_i = \frac{e^{z_i}}{\sum_{j=1}^{k} e^{z_j}} $

其中，e 是自然对数的底数，即欧拉数。

Softmax 的主要特点是对输入进行指数化（取 e的幂），然后进行归一化，得到一个概率分布。这种概率分布可以被解释为每个类别的估计概率。

社会科学家邓肯·卢斯于1959年在*选择模型*（choice model）的理论基础上 发明的*softmax函数*正是这样做的： softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持 可导的性质。 为了完成这一目标，我们首先对每个未规范化的预测求幂，这样可以确保输出非负。 为了确保最终输出的概率值总和为1，我们再让每个求幂后的结果除以它们的总和。如下式：

$o=xw+b$

$\hat y = softmax(o)$

![image-20231207164329070](https://hqyj-note-picture.oss-cn-beijing.aliyuncs.com/picture_bak/mypicture202312071643219.png)  

 $\hat y_j = \frac{exp(o_j)}{\sum_{k} exp(o_k)}$

里，对于所有的$j$总有0≤$\hat y$≤1。 因此，$\hat y$可以视为一个正确的概率分布。 softmax运算不会改变未规范化的预测$o$之间的大小次序，只会确定分配给每个类别的概率。 因此，在预测过程中，我们仍然可以用下式来选择最有可能的类别。

$argmax \quad \hat y = argmax \quad oj$

# 2.熵(Entropy)

## 2.1什么是熵？

==混乱程度，不确定性，还是信息量?==

不同的人对熵有不同的解释：混乱程度，不确定性，惊奇程度，不可预测性，信息量等等，面对如此多的解释，第一次接触时难免困惑。 熵究竟是什么？

信息论中熵的概念首次被==香农==提出，目的是寻找一种高效/无损地编码信息的方法：以编码后数据的平均长度来衡量高效性，平均长度越小越高效；同时还需满足“无损”的条件，即编码后不能有原始信息的丢失。这样，香农提出了熵的定义：==无损编码事件信息的最小平均编码长度==。

![image-20231208101329032](https://hqyj-note-picture.oss-cn-beijing.aliyuncs.com/picture_bak/mypicture202312081013281.png) 

## 2.2计算编码长度

上文提到了熵的定义：无损编码事件信息的最小平均编码长度。编码长度容易理解，但何来的最小，又何来的平均呢？下面以一个例子来说明：假设我们采用二进制编码北京的天气信息，并传输至纽约，其中北京的天气状态有4种可能，对应的概率如下图，每个可能性需要1个编码，北京的天气共需要4个编码。让我们采用3种编码方式，并对比下编码长度。不难发现，方式3编码长度最小，且是平均意义上的最小。方式3胜出的原因在于：对高可能性事件(Fine,Cloudy)用短编码，对低可能性事件(Rainy,Snow)用长编码。表中的3种方式，像是尝试的过程，那么能否直接计算出服从某一概率分布的事件的最小平均编码长度呢？还句话说，能不能直接计算熵？

| 编码方式 | Fine(50%) | Cloudy(25%) | Rainy(12.5%) | Snow(12.5%) |                 编码长度                  |
| :------: | :-------: | :---------: | :----------: | :---------: | :---------------------------------------: |
|  方式1   |    10     |     110     |      0       |     111     | $2*50\%+3*25\%+1*12.5\%+3*12.5\% = 2.25$  |
|  方式2   |     0     |     110     |      10      |     111     | $1*50\%+3*25\%+2*12.5\%+3*12.5\% = 1.875$ |
|  方式3   |     0     |     10      |     110      |     111     | $1*50\%+2*25\%+3*12.5\%+3*12.5\% = 1.75$  |

> 注：上述的1,2,3代表的是字符的个数，并不是二进制的值。

![image-20231208145508003](https://hqyj-note-picture.oss-cn-beijing.aliyuncs.com/picture_bak/mypicture202312081455497.png) 

## 2.3直接计算熵

假设一个信息事件有8种可能的状态，且各状态等可能性，即可能性都是12.5%=1/8。我们需要多少位来编码8个值呢？1位可以编码2个值(0或1)，2位可以编码2×2=4个值(00,01,10,11)，则8个值需要3位，2×2×2=8(000,001,010,011,100,101,110,111)。

| A(12.5%) | B(12.5%) | C(12.5%) | D(12.5%) | E(12.5%) | F(12.5%) | G(12.5%) | H(12.5%) |
| :------: | :------: | :------: | :------: | :------: | :------: | :------: | :------: |
|   000    |   001    |   010    |   011    |   100    |   101    |   110    |   111    |

我们不能减少任何1位，因为那样会造成歧义，同样我们也不要多于3位来编码8个可能的值。归纳来看，对于具有N种等可能性状态的信息，每种状态的可能性P = 1/N，编码该信息所需的最小编码长度为：

$log_2N = - log_2\frac{1}{N} = -log_2P$

那么计算平均最小长度，也就是熵的公式为：

$Entropy = - \sum_iP(i)log_2P(i)$

其中P(i)是第i个信息状态的可能性。

> 注1：
>
> 为什么在上的公式中需要再前面乘P(i)?
>
> 比如有两场球赛，第一场球赛球队的赢球概率都是0.5，第二次球赛各队的赢球概率0.99和0.01。
>
> 左侧球队实力详解更难抉择出来结果，熵越大。右侧实力悬殊比较大，熵越小。如果不乘直接拿
>
> 结果求和结果是2，右侧求和的结果是6.65。这和熵的定义是违背的。当乘各自的比例之后会发现
>
> 左侧的熵的结果是1，右侧熵的结果是0.08。这就是前面系数的由来。

![image-20231208170918855](https://hqyj-note-picture.oss-cn-beijing.aliyuncs.com/picture_bak/mypicture202312081709406.png)  

> 注2：
>
> 假设您有一个硬币，它有两面，正面和反面。您每次抛这个硬币，它有一半的概率是正面，一半的概率是反面。那么，您每次抛硬币，得到的信息量是多少呢？
>
> 根据信息量的公式，我们可以计算出：
>
> $I(正面)=log⁡_2\frac{1}{0.5}=1$
>
> $I(反面)=log⁡_2\frac{1}{0.5}=1$
>
> 也就是说，无论您抛出的是正面还是反面，您都得到了1比特（bit）的信息量。比特是信息的最小单位，它表示了一个二进制的选择，例如0或1，是或否，正面或反面。
>
> 那么，您每次抛硬币，得到的信息熵是多少呢？
>
> 根据信息熵的公式，我们可以计算出：
>
> $H(\text{硬币}) = - \sum_{x \in \{\text{正面, 反面}\}} P(x) \log_2 P(x)$
>
> $ H(\text{硬币}) = - \left(0.5 \cdot \log_2 0.5 + 0.5 \cdot \log_2 0.5\right)$
>
> $H(硬币)=−(−0.5−0.5)$
>
> $1H(硬币)=1$
>
> 也就是说，您每次抛硬币，得到的信息熵也是1比特。这意味着，您在抛硬币之前，对硬币的结果的不确定性是1比特。信息熵越大，说明您对结果的不确定性越大，反之亦然。

## 2.4熵的直观解释

那么熵的那些描述和解释(混乱程度，不确定性，惊奇程度，不可预测性，信息量等)代表了什么呢？

如果熵比较大(即平均编码长度较长)，意味着这一信息有较多的可能状态，相应的每个状态的可能性比较低；因此每当来了一个新的信息，我们很难对其作出准确预测，即有着比较大的混乱程度/不确定性/不可预测性。

并且当一个罕见的信息到达时，比一个常见的信息有着更多的信息量，因为它排除了别的很多的可能性，告诉了我们一个确切的信息。在天气的例子中，Rainy发生的概率为12.5%，当接收到该信息时，我们减少了87.5%的不确定性(Fine,Cloudy,Snow)；如果接收到Fine(50%)的消息，我们只减少了50%的不确定性。

# 3.交叉熵

## 3.1交叉熵损失函数，二分类交叉熵？

熟悉机器学习的人都知道分类模型中会使用交叉熵作损失函数，也一定对机器学习中猫分类器使用的二分类交叉熵印象深刻，但交叉熵究竟是什么？字面上看，交叉熵分两部分“交叉”和“熵”，首先回顾下熵的公式。

$Entropy = - \sum_iP(i)log_2P(i)$

同理对于连续的变量x的概率分布P(x)，熵的公式可以表示为：

$Entropy = - \int P(x)log_2P(x)dx$

在熵的公式中，对于离散变量和连续变量，我们都是计算了负的可能性的对数的期望，代表了该事件理论上的平均最小编码长度，所以熵的公式也可表示如下，公式中的x~P代表我们使用概率分布P来计算期望（这里的期望值是对随机变量 x按照概率分布 p 进行期望，因此写作（$\mathbb{E}_{x \sim P} $），熵又可以简写为H：

$ H(P) = \mathbb{E}_{x \sim P}[-\log P(x)] $

重要的事情再说一遍：“熵是服从某一特定概率分布事件的理论最小平均编码长度”，只要我们知道了任何事件的概率分布，我们就可以计算它的熵；那如果我们不知道事件的概率分布，又想计算熵，该怎么做呢？那我们来对熵做一个估计吧，熵的估计的过程自然而然的引出了交叉熵。

## 3.2熵的估计

假如我们现在需要预报北京天气，在真实天气发生之前，我们不可能知道天气的概率分布；但为了下文的讨论，我们需要假设：对北京天气做一段时间的观测后，可以得到真实的概率分布P。

在观测之前，我们只有预估的概率分布Q，使用估计得到的概率分布，可以计算估计的熵：

$EstimatedEntropy = \mathbb{E}_{x \sim Q}[-logQ(x)]$

如果Q是真实的概率分布，根据上述公式，我们已经得到了编码北京天气信息的最小平均长度；然而估计的概率分布为我们的公式引入了两部分的不确定性：

- 计算期望的概率分布是Q，与真实的概率分布P不同。
- 计算最小编码长度的概率是 -logQ，与真实的最小编码长度 -logP 不同。

因为估计的概率分布Q影响了上述两个部分(期望和编码长度)，所以得到的结果很可能错得离谱，也因此该结果和真实熵的对比无意义。和香农的目标一样，我们希望编码长度尽可能的短，所以我们需要对比我们的编码长度和理论上的最小编码长度(熵)。假设经过观测后，我们得到了真实概率分布P，在天气预报时，就可以使用P计算平均编码长度，实际编码长度基于Q计算，这个计算结果就是P和Q的交叉熵。这样，实际编码长度和理论最小编码长度就有了对比的意义。

$ CrossEntropy= \mathbb{E}_{x \sim P}[-\log Q(x)] $

$ Entropy= \mathbb{E}_{x \sim P}[-\log P(x)] $

## 3.3交叉熵>=熵

交叉熵使用H(P,Q)表示，意味着使用P计算期望，使用Q计算编码长度；所以H(P,Q)并不一定等于H(Q,P)，除了在P=Q的情况下，H(P,Q) = H(Q,P) = H(P)。

$H(P,Q) = \mathbb{E}_{x \sim P}[-\log Q(x)]$

有一点很微妙但很重要：对于期望，我们使用真实概率分布P来计算；对于编码长度，我们使用假设的概率分布Q来计算，因为它是预估用于编码信息的。因为熵是理论上的平均最小编码长度，所以交叉熵只可能大于等于熵。换句话说，如果我们的估计是完美的，即Q=P，那么有H(P,Q) = H(P)，否则，H(P,Q) > H(P)。

至此，交叉熵和熵的关系应该比较明确了，下面让我们看看为什么要使用交叉熵作分类损失函数。

## 3.4交叉熵作为损失函数

假设一个动物照片的数据集中有5种动物，且每张照片中只有一只动物，每张照片的标签都是one-hot编码。

| Animal |     Dog     |     Fox     |    Horse    |    Eagle    |  Squirrel   |
| :----: | :---------: | :---------: | :---------: | :---------: | :---------: |
| Label  | [1,0,0,0,0] | [0,1,0,0,0] | [0,0,1,0,0] | [0,0,0,1,0] | [0,0,0,0,1] |

第一张照片是狗的概率为100%，是其他的动物的概率是0；第二张照片是狐狸的概率是100%，是其他动物的概率是0，其余照片同理；因此可以计算下，每张照片的熵都为0。换句话说，以one-hot编码作为标签的每张照片都有100%的确定度，不像别的描述概率的方式：狗的概率为90%，猫的概率为10%。

假设有两个机器学习模型对第一张照片分别作出了预测：Q1和Q2,而第一张照片的真实标签为[1,0,0,0,0]:

| Model |          Prediction          |
| :---: | :--------------------------: |
|  Q1   | [0.4，0.3，0.005，0.05，0.2] |
|  Q2   |   [0.98，0.01，0，0，0.01]   |

两个模型预测效果如何呢，可以分别计算下交叉熵：

$H(P1,Q1) = - \sum_iP_1(i)logQ_1(i) = -(1log0.4+0log0.3+0log0.05+0log0.05+0log0.2) \approx 0.916$

$H(P1,Q2) = - \sum_iP_1(i)logQ_2(i) = -(1log0.98+0log0.01+0log0+0log0+0log0.01) \approx 0.02$ 

交叉熵对比了模型的预测结果和数据的真实标签，随着预测越来越准确，交叉熵的值越来越小，如果预测完全正确，交叉熵的值就为0（如果熵不为0，更接近于真实的熵）。因此，训练分类模型时，可以使用交叉熵作为损失函数。

## 3.5二分类交叉熵

在二分类模型中，标签只有是和否两种；这时，可以使用二分类交叉熵作为损失函数。假设数据集中只有猫和狗的照片，则交叉熵公式中只包含两种可能性：

$H(P,Q)=- \sum_{i=(cat,dog)}P(i)logQ(i) = -P(cat)logQ(cat)-P(dog)logQ(dog)$

又因为：

$P(cat)=1-P(dog)$

$Q(dog)=1-Q(cat)$

所以交叉熵可以表示为：

$H(P,Q) = -P(cat)logQ(cat)-(1-P(cat))log(1-Q(cat))$

使用如下定义：

$P = P(cat)$

$\overset{\sim}{P} = Q(cat)$

二分类的交叉熵可以写作如下形式，看起来就熟悉多了。

$BinaryCrossEntropy=-Plog\overset{\sim}{P}-(1-P)log(1-\overset{\sim}{P})$

> 在机器学习中，常见的交叉熵损失函数有两种形式：二分类交叉熵（Binary Cross-Entropy）和多分类交叉熵（Categorical Cross-Entropy）。
>
> **二分类交叉熵**： 对于二分类问题，损失函数定义为：
>
> $$ H(y, \hat{y}) = - (y \log(\hat{y}) + (1 - y) \log(1 - \hat{y})) $$
>
> 其中 $y$是真实标签，$\hat{y}$是模型的预测值。
>
> 
>
> **多分类交叉熵**： 对于多分类问题，损失函数定义为：
>
> $$ H(y, \hat{y}) = - \sum_{i} y_i \log(\hat{y}_i) $$
>
> 其中 $y_i$是真实标签第$i$个元素，$\hat{y}_i$是模型的预测值第$i$个元素。
>
> 在训练神经网络时，我们的目标是最小化交叉熵损失，从而使模型的输出尽可能接近真实的概率分布。这样的训练过程有助于提高模型对不同类别的分类准确性。